{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入函数库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T03:58:24.385913Z",
     "start_time": "2020-06-04T03:58:23.274863Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras  # 在TF2.0版本中,已经是自带Keras了,所以不需要额外安装\n",
    "import tensorflow_datasets as tfds # 这个是之前说过的Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如果出现显存不够的错误,把这个代码加上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T03:58:24.409965Z",
     "start_time": "2020-06-04T03:58:24.387015Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加入下面这个代码片段主要是因为TF2.0对RTX20系列显卡支持不是很好,容易爆显存,所以设置成用多少占多少显存,而不是一次性全占了(默认)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T03:58:24.427088Z",
     "start_time": "2020-06-04T03:58:24.410898Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = tf.keras.layers\n",
    "models = tf.keras.models\n",
    "keras_utils = tf.keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T03:58:24.434829Z",
     "start_time": "2020-06-04T03:58:24.430027Z"
    }
   },
   "outputs": [],
   "source": [
    "WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                       'releases/download/v0.2/'\n",
    "                       'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')  # 此处的预训练模型不带全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50模型改写用于Cifar100分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T03:58:24.443544Z",
     "start_time": "2020-06-04T03:58:24.436987Z"
    }
   },
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = 3\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size,\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T03:58:24.451995Z",
     "start_time": "2020-06-04T03:58:24.444501Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2)):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = 3\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters2, kernel_size, padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters3, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = layers.BatchNormalization(\n",
    "        axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T03:58:24.462582Z",
     "start_time": "2020-06-04T03:58:24.453493Z"
    }
   },
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(32, 32, 3),\n",
    "         classes=100):\n",
    "    img_input = layers.Input(shape=input_shape)  # 输入节点\n",
    "    \n",
    "    bn_axis = 3\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='conv1')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu', name='fc1')(x)\n",
    "    x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    inputs = img_input  # inputs是输入节点\n",
    "    outputs = x # x是输出节点\n",
    "    model = models.Model(inputs, outputs , name='resnet50')  # 生成一个Model, 需要指定输入和输出\n",
    "    \n",
    "    weights_path = keras_utils.get_file(\n",
    "        'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',  # 此处加载的是不存在全连接层的预训练模型\n",
    "        WEIGHTS_PATH_NO_TOP,\n",
    "        cache_subdir='models',\n",
    "        file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "    model.load_weights(weights_path, by_name=True)  \n",
    "    # 加载在ImageNet上预训练过的模型，注意by_name参数很有用，把layer和layer name对应上了\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 声明一个ResNet50模型实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T04:01:12.944343Z",
     "start_time": "2020-06-04T03:58:24.463439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 164s 2us/step\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练日志记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T04:01:13.086932Z",
     "start_time": "2020-06-04T04:01:12.978129Z"
    }
   },
   "outputs": [],
   "source": [
    "log_dir = './logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T04:01:13.095666Z",
     "start_time": "2020-06-04T04:01:13.089743Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]\n",
    "    return image, label\n",
    "\n",
    "def augment(image,label):\n",
    "    image,label = convert(image, label)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取并预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T04:01:13.271313Z",
     "start_time": "2020-06-04T04:01:13.096564Z"
    }
   },
   "outputs": [],
   "source": [
    "# 此处使用tfds的方式构建data pipeline\n",
    "(raw_test, raw_train), metadata = tfds.load(\n",
    "    'cifar100', # 数据集名称，这个是cifar100分类数据集，共100个类别\n",
    "    split=['test', 'train'], # 这里的raw_test和split的'test'对应，raw_train和split的'train'对应\n",
    "    with_info=True, # 这个参数和metadata对应\n",
    "    as_supervised=True, # 这个参数的作用是返回tuple形式的(input, label),举个例子，raw_test=tuple(input, label)\n",
    "    shuffle_files=True,\n",
    "    data_dir='./tensorflow_datasets'\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE_BUFFER_SIZE = 25000\n",
    "\n",
    "# 可以体验下这里是否加prefetch(tf.data.experimental.AUTOTUNE)和cache()的区别，对训练速度，以及CPU负载有影响\n",
    "train_batches = raw_train.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).map(augment).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_batches = raw_test.map(convert).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T04:27:12.546099Z",
     "start_time": "2020-06-04T04:01:13.272192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3125/3125 [==============================] - 76s 24ms/step - loss: 4.5772 - accuracy: 0.0489\n",
      "Epoch 2/20\n",
      "3125/3125 [==============================] - 76s 24ms/step - loss: 4.5234 - accuracy: 0.1051\n",
      "Epoch 3/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.4842 - accuracy: 0.14220s - los\n",
      "Epoch 4/20\n",
      "3125/3125 [==============================] - 79s 25ms/step - loss: 4.4602 - accuracy: 0.1639\n",
      "Epoch 5/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.4448 - accuracy: 0.1792\n",
      "Epoch 6/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.4306 - accuracy: 0.1933\n",
      "Epoch 7/20\n",
      "3125/3125 [==============================] - 77s 25ms/step - loss: 4.4197 - accuracy: 0.2039\n",
      "Epoch 8/20\n",
      "3125/3125 [==============================] - 76s 24ms/step - loss: 4.4056 - accuracy: 0.2185\n",
      "Epoch 9/20\n",
      "3125/3125 [==============================] - 77s 25ms/step - loss: 4.3887 - accuracy: 0.2358\n",
      "Epoch 10/20\n",
      "3125/3125 [==============================] - 76s 24ms/step - loss: 4.3777 - accuracy: 0.2474\n",
      "Epoch 11/20\n",
      "3125/3125 [==============================] - 81s 26ms/step - loss: 4.3664 - accuracy: 0.2585\n",
      "Epoch 12/20\n",
      "3125/3125 [==============================] - 77s 25ms/step - loss: 4.3578 - accuracy: 0.2669\n",
      "Epoch 13/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.3509 - accuracy: 0.2732\n",
      "Epoch 14/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.3428 - accuracy: 0.2819\n",
      "Epoch 15/20\n",
      "3125/3125 [==============================] - 79s 25ms/step - loss: 4.3385 - accuracy: 0.2859\n",
      "Epoch 16/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.3321 - accuracy: 0.2921\n",
      "Epoch 17/20\n",
      "3125/3125 [==============================] - 76s 24ms/step - loss: 4.3292 - accuracy: 0.2949\n",
      "Epoch 18/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.3255 - accuracy: 0.2983\n",
      "Epoch 19/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.3213 - accuracy: 0.3030\n",
      "Epoch 20/20\n",
      "3125/3125 [==============================] - 78s 25ms/step - loss: 4.3184 - accuracy: 0.3058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f43ac052160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_batches,\n",
    "    epochs=200,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline的test acc，并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T04:27:18.800974Z",
     "start_time": "2020-06-04T04:27:12.547333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 5s 8ms/step - loss: 4.3315 - accuracy: 0.2905\n",
      "Baseline test accuracy:  0.2904999852180481\n",
      "Saved baseline model to:  ./test.h5\n"
     ]
    }
   ],
   "source": [
    "# 此处我们可以看到Baseline的test acc和inference速度为6ms/step\n",
    "_, baseline_model_accuracy = model.evaluate(test_batches, verbose=1)\n",
    "print('Baseline test accuracy: ', baseline_model_accuracy)\n",
    "\n",
    "keras_file = './test.h5'\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to: ', keras_file)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "498.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
