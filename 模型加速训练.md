# Tensorflow2.2

![Selection_011](pics/Selection_011.jpg)

我们来观察一下InceptionV3Classification(单机单卡)在epochs=30的情况下，随着batch_size=32, 64, 128的增大，训练的elapsedTime=7870, 7065, 6657在减少，这是因为单卡2080Ti的性能在batch_size=32的时候没完全发挥出来，直到batch_size=128才完全压榨出来，再大的话，就要显存溢出了，同时我们也可以看到trainAcc随着batch_size的增大也在增大，这说明更大的batch_size可以加快模型收敛速度

在单机单卡的实验前提下，我们进行分布式训练，很自然的，我们选择单卡batch_size=128，那么双卡就是128*2=256，对比两者的elapsedTime和trainAcc我们发现，双卡训练表现出了很大的优势，时间方面的话(6657 - 5376) / 6657 = 0.19，也就是提速19%，准确率方面的话0.44 - 0.354 = 0.08，相当于高了8%，这是建立在训练相同的30个epoch前提下，其实正确的对比应该是达到相同精度，单卡和多卡所花费时间进行对比

我们还可以发现一个特殊的地方，单卡batch_size=32时，elapsedTime=7870，双卡batch_size=32*2时，elapsedTime=8235，这个是因为多卡所带来的计算速度增益小于卡间通信所带来的开销，形象点解释，一个人干活慢，两个人干活(如果配合好，分工干活快，如果配合不好，可能干活速度比一个人还慢)

## Tesla服务器显卡(V100双卡)

![Selection_010](pics/Selection_010.jpg)

![Selection_009](pics/Selection_009.jpg)

![Selection_008](pics/Selection_008.jpg)

我们发现，随着训练的进行，每个epoch所花费的时间大概是在103s，显卡的利用率基本维持在95%以上，显卡的显存也是打满的，显卡的温度维持在60度左右

此外还可以把内存加载成硬盘使用**sudo mount tmpfs test -t tmpfs -o size=25G**

上面的test需要我们自己在本地创建，运行mkdir test就可以

## RTX消费级显卡(RTX2080Ti双卡)

![Selection_012](pics/Selection_012.jpg)

![Selection_007](pics/Selection_007.jpg)

![Selection_013](pics/Selection_013.jpg)

我们发现，随着训练的进行，每个epoch所花费的时间，一开始是175s，之后是189s，然后是196s，最后大致稳定在199s，显卡的利用率基本维持在95%以上，显卡的显存也是打满的，显卡温度维持在80度左右

对比V100和RTX2080Ti的每个epoch训练时间，我们可以发现云服务还是有优势的，主要体现在稳定性，比如降温处理等，而且对于长时间训练来说，服务器显卡表现的更加稳定

## 数据读取所用的tfds

```
train_batches = raw_train.shuffle(args.shuffle_buffer_size).map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE, deterministic=False).batch(args.batch_size).prefetch(tf.data.experimental.AUTOTUNE)
```

第一个操作是进行shuffle，如果可以的话，这里的buffer_size我们尽量选小点，主要是我们需要加载一定量的样本进内存，之后再进行数据扰动，这个需要一定时间等待

第二个操作是进行数据增强，这里的话，我们设置了num_parallel_calls，官方原话是这样的，Performance can often be improved by setting num_parallel_calls so that map will use multiple threads to process elements，如果不设置的话，elements will be processed sequentially，我们这里采用tf.data.experimental.AUTOTUNE的好处是可以自适应不同硬件环境，还有一个点是设置deterministic=False，这个的意思是不采用确定性卷积(如果采用确定性卷积，对模型复现有帮助，但相应的会降低性能)，可以提高一定的速度

第三个是进行预读取，官方原话是，this allows later elements to be prepared while the current element is being processed，也就是说你的模型正在处理数据的时候，下一批需要处理的数据，已经提前给你准备好了

其实在实际训练模型的时候，特别是多卡的时候，很常见的问题就是CPU处理数据的速度跟不上GPU处理数据的速度，这个也就是我们经常说的I/O瓶颈，这也是我为什么使用tfds的原因，数据在本地的存储格式是TFRecord(二进制格式文件)，这些都能用来解决I/O瓶颈问题

# Pytorch1.5

## Tesla服务器显卡(V100双卡)

![Selection_029](pics/Selection_029.jpg)

setting pin_memory=True，which enables fast data transfer to CUDA-enabled GPUs

num_workers>0，开启多进程数据加载

我发现8核处理器的时候，GPU利用率波动更加厉害，说明数据传输更不上GPU处理的数据，更换到16核之后，好了很多，也就是下面两个截图所示

![Selection_020](pics/Selection_020.jpg)

![Selection_021](pics/Selection_021.jpg)

![Selection_023](pics/Selection_023.jpg)

通过上图，我们发现仅仅进行训练，并且训练过程中不计算trainAcc的情况下，pytorch的双卡V100还是比tensorflow双卡慢，每个epoch耗时115s，而且看GPU利用率就会发现，抖动的比较厉害，而且在每个epoch训练完之后，会出现几秒的空闲时间

## 混合精度训练(Apex)

![Selection_028](../Pictures/Selection_028.jpg)

![Selection_026](pics/Selection_026.jpg)

![Selection_027](pics/Selection_027.jpg)

使用混合精度训练之后，我们发现，速度还是比较理想的，每个epoch耗时102s

更深一步的优化可以使用Nvidia的DALI，把数据预处理的一部分工作放到GPU上处理，同时比较推荐的方案是Distributed Data Parallel + Apex方案