# Pruning

## Trim insignificant weights

[Trim insignificant weights](https://www.tensorflow.org/model_optimization/guide/pruning)

Train a CNN model on the MNIST handwritten digit classification task with pruning: [code](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/examples/sparsity/keras/mnist/mnist_cnn.py)

Train a LSTM on the IMDB sentiment classification task with pruning: [code](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/examples/sparsity/keras/imdb/imdb_lstm.py)

To prune, or not to prune: exploring the efficacy of pruning for model compression: [paper](https://arxiv.org/pdf/1710.01878.pdf)

![Selection_004](pics/Selection_004.png)

​					上图仅供参考，最好还是需要自己再去复现一遍，**待填坑**，**这两个网络看情况吧**

## Pruning in Keras example

[Pruning in Keras example](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#see_persistence_of_accuracy_from_tf_to_tflite)

| 数据集 | 模型                                     | 模型体积 | 准确率 |
| ------ | ---------------------------------------- | -------- | ------ |
| mnist  | baseline(简单模型)                       | 1x       | 0.9808 |
| mnist  | baseline_Wpruned(简单模型)               | 3x压缩   | 0.9699 |
| mnist  | baseline_Wpruned_Wquantization(简单模型) | 10x压缩  | 0.9698 |

TF ------>  TFLite    对模型进行量化(quantization)

create sparse models with the TensorFlow Model Optimization Toolkit API for both Tensorflow and TFLite

### 经验总结

代码在**tutorial/tf2_pruning/pruning_with_keras.ipynb**，**这个代码的话得和Magnitude-weightPruning.ipynb进行一下整合**

#### CUDA方面的显存报错，在RTX20系列显卡上

![Selection_048](pics/Selection_048.png)

#### 获得模型压缩后的体积大小以及zip文件

![Selection_058](pics/Selection_058.png)

#### 定义一个用于评估TFLite模型的函数(这个应该属于量化的)

![Selection_059](pics/Selection_059.png)

#### 利用tensorboard可视化每层的稀疏度以及训练的acc和loss

![Selection_060](pics/Selection_060.png)

![Selection_066](pics/Selection_066.png)

#### 训练的时候batch_size大小适当的话，可以加快训练速度

![Selection_061](pics/Selection_061.png)

​										上图的batch_size=128，每个epoch耗时1s

![Selection_064](pics/Selection_064.png)

​										上图的batch_size=32，每个epoch耗时4s

这种情况一般是在显卡内存没有打满，GPU使用率没有打满的情况下可以使用，如下图所示

![Selection_065](pics/Selection_065.png)

## Pruning comprehensive guide

[Pruning comprehensive guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide)



# Lightweight neural network

初步定的是ShuffleNet和MobileNet

可以从MAC(内存访问成本)

FLOPS: 指每秒浮点运算次数，可以理解为计算的速度. 是衡量硬件性能的一个指标

FLOPs: 指浮点运算数，理解为计算量. 可以用来衡量算法/模型的复杂度. (模型)在论文中常用GFLOPs(1 GFLOPs = 10^9 FLOPs)

[CNN模型所需的计算力(FLOPs)和参数量(parameters)是怎么计算的](https://www.zhihu.com/question/65305385)

参考链接: 

+ [轻量级神经网络(一)--ShuffleNetV2](https://zhuanlan.zhihu.com/p/67009992)
+ [MobileNetv1~v3, ShuffleNet等轻量级网络](https://cygao.xyz/2019/07/12/lightweight/)
+ [轻量级神经网络(二)--MobileNetv1~v3](https://zhuanlan.zhihu.com/p/70703846)
+ [精简网络模型(一)](https://zhuanlan.zhihu.com/p/65998279)
+ [精简CNN模型以ShuffleNetv2为例子解释, 移动端网络设计的需要的规则ARM、GPU上对比, 各种卷积方式的演变跟介绍](https://blog.csdn.net/yangdashi888/article/details/87912600)
+ [精简网络模型(二)](https://zhuanlan.zhihu.com/p/66230470)



