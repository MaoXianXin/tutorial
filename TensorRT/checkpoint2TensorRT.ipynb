{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把checkpoint转化成frozen graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "import sys\n",
    "sys.path.append('/home/mao/Github/models/research/slim')\n",
    "import nets.nets_factory\n",
    "SAVED_MODEL_DIR = \"/home/mao/Github/tensorRT/resnet_v1_50_2016_08_28/\"\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=config) as sess:\n",
    "        tf_input = tf.placeholder(tf.float32, [None, 224, 224, 3], name='input')\n",
    "        network_fn = nets.nets_factory.get_network_fn('resnet_v1_50', 1000,\n",
    "                                                      is_training=False)\n",
    "        tf_net, tf_end_points = network_fn(tf_input)\n",
    "                \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, SAVED_MODEL_DIR+\"resnet_v1_50.ckpt\")\n",
    "        \n",
    "        tf_output = tf.identity(tf_net, name='logits')\n",
    "        tf_output_classes = tf.argmax(tf_output, axis=1, name='classes')        \n",
    "        #tf_output_classes = tf.reshape(tf_output_classes, (BATCH_SIZE,), name='classes')\n",
    "        \n",
    "        # freeze graph\n",
    "        fp32_frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            sess.graph_def,\n",
    "            output_node_names=['logits', 'classes']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试frozen graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_frozen_graph(frozen_graph, SAVED_MODEL_DIR=None, BATCH_SIZE=8):\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        next_element = tf.convert_to_tensor(np.random.random((8, 224, 224, 3)))\n",
    "\n",
    "        output_node = tf.import_graph_def(\n",
    "            frozen_graph,\n",
    "            return_elements=['classes'],\n",
    "            name=\"\")\n",
    "        \n",
    "        print('Warming up for 50 batches...')\n",
    "        for _ in range (50):\n",
    "            sess.run(['classes:0'], feed_dict={\"input:0\": sess.run(next_element)})\n",
    "\n",
    "        num_predict = 0\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            for i in range(0, 1000):        \n",
    "                image_data = sess.run(next_element)    \n",
    "                img = image_data\n",
    "                output = sess.run(['classes:0'], feed_dict={\"input:0\": img})\n",
    "                num_predict += len(output[0])\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            pass\n",
    "\n",
    "        print('Inference speed: %.2f samples/s'%(num_predict/(time.time()-start_time)))\n",
    "        \n",
    "        #Optionally, save model for serving if an ouput directory argument is presented\n",
    "        if SAVED_MODEL_DIR:\n",
    "            print('Saving model to %s'%SAVED_MODEL_DIR)\n",
    "            tf.saved_model.simple_save(\n",
    "                session=sess,\n",
    "                export_dir=SAVED_MODEL_DIR,\n",
    "                inputs={\"input\":tf.get_default_graph().get_tensor_by_name(\"input:0\")},\n",
    "                outputs={\"classes\":tf.get_default_graph().get_tensor_by_name(\"classes:0\")},\n",
    "                legacy_init_op=None\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存FP32,并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Inference speed: 245.79 samples/s\n",
      "Saving model to /home/mao/Github/tensorRT/model/Resnet_FP32/\n"
     ]
    }
   ],
   "source": [
    "FP32_SAVED_MODEL_DIR = \"/home/mao/Github/tensorRT/model/Resnet_FP32/\"\n",
    "benchmark_frozen_graph(fp32_frozen_graph, FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存tensorRT的FP32,并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create the TFTRT FP32 engine\n",
    "BATCH_SIZE = 8\n",
    "converter = trt.TrtGraphConverter(input_graph_def=fp32_frozen_graph,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.FP32,\n",
    "                                  nodes_blacklist=['classes', 'logits'])\n",
    "trt_fp32_graph = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Inference speed: 328.11 samples/s\n",
      "Saving model to /home/mao/Github/tensorRT/model/Resnet_TRT_FP32/\n"
     ]
    }
   ],
   "source": [
    "TRT_FP32_SAVED_MODEL_DIR = \"/home/mao/Github/tensorRT/model/Resnet_TRT_FP32/\"\n",
    "benchmark_frozen_graph(trt_fp32_graph, TRT_FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存tensorRT的INT8,并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 224, 224, 3) (8, 224, 224, 3)\n",
      "(8, 224, 224, 3) (8, 224, 224, 3)\n",
      "Calibration data shape:  (16, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 2\n",
    "BATCH_SIZE = 8\n",
    "batched_input = np.zeros((BATCH_SIZE * num_calibration_batches, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "    # prepare dataset iterator\n",
    "    next_element = tf.convert_to_tensor(np.random.random((8, 224, 224, 3)))\n",
    "    for i in range(num_calibration_batches):\n",
    "        print(batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :].shape, sess.run(next_element).shape)\n",
    "        batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :] = sess.run(next_element)\n",
    "\n",
    "#batched_input = tf.constant(batched_input)\n",
    "print('Calibration data shape: ', batched_input.shape)\n",
    "\n",
    "def calibration_input_fn_gen():\n",
    "    for i in range(num_calibration_batches):\n",
    "        yield batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]\n",
    "        \n",
    "calibration_input_fn = calibration_input_fn_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create the TFTRT FP16 engine\n",
    "converter = trt.TrtGraphConverter(input_graph_def=fp32_frozen_graph,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.INT8,\n",
    "                                  nodes_blacklist=['classes', 'logits'])\n",
    "trt_int8_graph = converter.convert()\n",
    "\n",
    "\n",
    "# Run calibration for num_calibration_batches times.\n",
    "trt_int8_calibrated_graph = converter.calibrate(\n",
    "      fetch_names=['classes:0'],\n",
    "      num_runs=num_calibration_batches,\n",
    "      feed_dict_fn=lambda: {\"input:0\": next(calibration_input_fn)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Inference speed: 503.99 samples/s\n",
      "Saving model to /home/mao/Github/tensorRT/model/Resnet_TRT_INT8/\n"
     ]
    }
   ],
   "source": [
    "INT8_SAVED_MODEL_DIR = \"/home/mao/Github/tensorRT/model/Resnet_TRT_INT8/\"\n",
    "benchmark_frozen_graph(trt_int8_calibrated_graph, INT8_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分别保存frozen_graph的.pb和tensorRT的FP32和INT8的.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import graph_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/fp32_frozen_graph.pb'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_io.write_graph(fp32_frozen_graph, './model', 'fp32_frozen_graph.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/trt_fp32_graph.pb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_io.write_graph(trt_fp32_graph, './model', 'trt_fp32_graph.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/trt_int8_calibrated_graph.pb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_io.write_graph(trt_int8_calibrated_graph, './model', 'trt_int8_calibrated_graph.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
